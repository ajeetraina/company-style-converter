version: '3.8'

services:
  app:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    restart: unless-stopped
    ports:
      - "5000:5000" # Backend API
      - "3000:3000" # Frontend
    volumes:
      - ../uploads:/app/uploads
      - ../company-templates:/app/company-templates
    environment:
      - NODE_ENV=production
      - PORT=5000
      - MCP_ENDPOINT=${MCP_ENDPOINT:-http://model-runner.docker.internal/engines/llama.cpp/v1}
      - MODEL_RUNNER_HOST=model-runner.docker.internal
      - MODEL_RUNNER_PORT=80
      - MODEL_RUNNER_ENGINE=llama.cpp
      - DEFAULT_MODEL=ai/llama3.2:1B-Q8_0
    networks:
      - app-network

  # Optional: Proxy for direct TCP access to Model Runner
  model-runner-proxy:
    image: alpine/socat
    restart: unless-stopped
    command: tcp-listen:80,fork,reuseaddr tcp:host.docker.internal:12434
    ports:
      - "8080:80"
    networks:
      - app-network

networks:
  app-network:
    driver: bridge
